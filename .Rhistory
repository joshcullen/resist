#South
rast.S<- raster(ext=extent(c(min(dat.S$x) - 30, max(dat.S$x) + 30, min(dat.S$y) - 30,
max(dat.S$y + 30))),
crs = "+init=epsg:32721",
res = 30)
values(rast.S)<- 0
lulcS_30m<- crop(lulcS_30m, rast.S)
plot(lulcS_30m); points(dat.S$x, dat.S$y)
#slope
slope.N = terrain(dem_N.tif, opt='slope', unit = "degrees")
slope.S = terrain(dem_S.tif, opt='slope', unit = "degrees")
#resample DEMs to 30m from 18m; they will now share the same dimensions and extent
slope.N<- resample(slope.N, lulcN_30m, method = "bilinear")
compareRaster(lulcN_30m, slope.N)  #check if same extent, dimensions, projection, resolution,
#and origin
plot(slope.N); points(dat.N$x, dat.N$y)
#resample DEMs to 30m from 18m; they will now share the same dimensions and extent
slope.S<- resample(slope.S, lulcS_30m, method = "bilinear")
compareRaster(lulcS_30m, slope.S)
plot(slope.S); points(dat.S$x, dat.S$y)
setwd("~/Documents/Snail Kite Project/Data/armadillos/NDVI")
#load files
ndvi.filenames<- list.files(getwd(), pattern = "*.grd$")
ndvi.N<- brick(ndvi.filenames[1])
ndvi.S<- brick(ndvi.filenames[2])
ndvi.N
ndvi.S
#change extent and dimensions of RasterBricks using resample()
ndvi.N<- resample(ndvi.N, lulcN_30m, method = "bilinear")
compareRaster(dist2rdN_30m, ndvi.N)
compareRaster(lulcN_30m, ndvi.N)
plot(ndvi.N[[1]])
points(dat.N$x, dat.N$y)
#change extent and dimensions of RasterBricks using resample()
ndvi.S<- resample(ndvi.S, lulcS_30m, method = "bilinear")
compareRaster(lulcS_30m, ndvi.S)
plot(ndvi.S[[1]])
points(dat.S$x, dat.S$y)
### *TEMPORARY* take mean NDVI for all rasters over study period at each site
ndvi.N.mean<- mean(ndvi.N, na.rm = T)
ndvi.S.mean<- mean(ndvi.S, na.rm = T)
covars.N<- brick(lulcN_30m, slope.N, ndvi.N.mean)
names(covars.N)<- c("lulc", "slope", "ndvi")
covars.S<- brick(lulcS_30m, slope.S, ndvi.S.mean)
names(covars.S)<- c("lulc", "slope", "ndvi")
covars.N
covars.S
?`extract,Raster,data.frame-method`
View(dat.N)
path.N<- extract(covars.N, dat.N[,c("x","y")])
View(path.N)
path.N<- cbind(dat.N, path.N)
View(path.N)
path.S<- extract(covars.S, dat.S[,c("x","y")])
path.S<- cbind(dat.S, path.S)
View(path.S)
unique(path.N$lulc)
unique(path.S$lulc)
classes_DL_padrao.tif@data@attributes[[1]]$SPRCLASSE
classes_DL_padrao.tif
classes_DL_padrao.tif@data
setwd("~/Documents/Snail Kite Project/Data/armadillos/Environ Data")
rast<- dir(getwd(), "*.tif$")
for (i in rast) assign(i, raster(i))
classes_DL_padrao.tif@data@attributes[[1]]$SPRCLASSE
# Provide names to numbers for LULC
path.N<- path.N %>%
mutate_at("lulc", ~recode(., '1' = 'Pasto',
'2' = 'Sede',
'3' = 'Cerca',
'4' = 'Agua',
'5' = 'Cana',
'6' = 'Mata'))
View(path.N)
path.S<- path.S %>%
mutate_at("lulc", ~recode(., '1' = 'Campo',
'3' = 'Mata',
'4' = 'Agua',
'5' = 'Pasto',
'7' = 'Estrada'))
View(path.S)
setwd("~/Documents/Snail Kite Project/Data/armadillos/Environ Data")
#Load and wrangle data
temp<- read.csv("caceres_corumba_2014e15.csv", as.is = T)
temp<- temp %>%
mutate(date.round = as.POSIXct(strptime(paste(data, hora.utc), format = "%d-%m-%Y %H",
tz = "UTC")),
.before = everything())
#Create new col to store rounded datetimes
path.N<- path.N %>%
mutate(date.round = round_date(path.N$date, unit = "hour"), .before = state)
View(path.N)
#Create new col to store rounded datetimes
path.N<- path.N %>%
mutate(date.round = round_date(path.N$date, unit = "hour"), .before = dx)
path.S<- path.S %>%
mutate(date.round = round_date(path.S$date, unit = "hour"), .before = dx)
#Merge data
path.N<- left_join(path.N, temp[,c("date.round","t.ar","rain")], by = "date.round") %>%
dplyr::select(-date.round)
path.S<- left_join(path.S, temp[,c("date.round","t.ar","rain")], by = "date.round") %>%
dplyr::select(-date.round)
View(path.S)
path<- rbind(path.N, path.S)
View(path)
setwd("~/Documents/Snail Kite Project/Data/R Scripts/ValleLabUF/resist")
write.csv(path, "Armadillo Environ Data_NoBurrow.csv", row.names = F)
path.N<- extract(covars.N, dat.N[,c("x","y")], along = TRUE)
View(path.N)
library(elevatr)
library(tidyverse)
library(sf)
library(raster)
library(lubridate)
library(sp)
###################
### Import data ###
###################
setwd("~/Documents/Snail Kite Project/Data/armadillos")
dat<- read.csv("Modified Armadillo Data.csv", header = T, sep = ",")
dat$id<- as.character(dat$id)
dat$date<- as_datetime(dat$date)
# Separate tracks by region (N or S)
dat.N<- dat %>% filter(region == "N")
dat.S<- dat %>% filter(region == "S")
# Create dummy rasters for extent of each region (with buffer)
#North
rast.N<- raster(ext=extent(c(min(dat.N$x) - 30, max(dat.N$x) + 30, min(dat.N$y) - 30,
max(dat.N$y + 30))),
crs = "+init=epsg:32721",
res = 30)
values(rast.N)<- 0
#South
rast.S<- raster(ext=extent(c(min(dat.S$x) - 30, max(dat.S$x) + 30, min(dat.S$y) - 30,
max(dat.S$y + 30))),
crs = "+init=epsg:32721",
res = 30)
values(rast.S)<- 0
#check that points fit
plot(rast.N)
points(dat.N$x, dat.N$y)
plot(rast.S)
points(dat.S$x, dat.S$y)
dem.N<- get_elev_raster(rast.N, z = 12) #18 m res
?get_elev_raster
set.seed(1)
library(tidyverse)
library(momentuHMM)
library(lubridate)
library(tictoc)
setwd("~/Documents/Snail Kite Project/Data/armadillos")
dat<- read.csv("Modified Armadillo Data.csv", header = T, sep = ",")
dat$id<- as.character(dat$id)
dat$date<- as_datetime(dat$date)
tmp<- which(dat$InBurrow == 1)
dat$y[tmp]<- dat$y[tmp] + runif(length(tmp), -0.5,0.5)
dat$x[tmp]<- dat$x[tmp] + runif(length(tmp), -0.5,0.5)
#prep data for use by `momentuHMM`
dat2<- dat %>% rename(ID = id)
dat2<- bayesmove::round_track_time(dat2, "ID", 300, 60)
dat2<- filter(dat2, dt == 300)
dat.prep<- prepData(dat2, type = "UTM", coordNames = c("x", "y"))
## K = 3
allm<- list()
niter<- 30
stateNames <- c("Burrow","Foraging","Transit")
whichzero <- which(dat.prep$step == 0)
propzero <- length(whichzero)/nrow(dat.prep)
zeromass0 <- c(propzero, 0, 0)        #for zero distances by state
tic()
for (i in 1:niter) {
print(paste("Iteration", i))
# Step length mean
stepMean0 <- runif(3,
min = c(0.01, 3, 20),
max = c(2, 15, 40))
# Step length standard deviation
stepSD0 <- runif(3,
min = c(0.01, 2, 20),
max = c(2, 10, 40))
# Turning angle mean
angleMean0 <- runif(3,
min = c(3*pi/4, pi/2, 0),
max = c(pi, pi, pi/4))
# Turning angle concentration
angleCon0 <- runif(3,
min = c(0.5, 0.3, 0.6),
max = c(0.9, 0.7, 0.9))
# Fit model
if(propzero > 0) {  #don't include zero mass if no 0s present
stepPar0 <- c(stepMean0, stepSD0, zeromass0)
} else {
stepPar0 <- c(stepMean0, stepSD0)
}
anglePar0 <- c(angleMean0, angleCon0)
allm[[i]]<- fitHMM(data = dat.prep, nbStates = 3,
Par0 = list(step = stepPar0, angle = anglePar0),
dist = list(step = "gamma", angle = "wrpcauchy"),
formula = ~ 1, stationary=TRUE, #stationary for a slightly better fit
estAngleMean = list(angle=TRUE),
stateNames = stateNames,
optMethod = "Nelder-Mead")
}
toc()
# Extract likelihoods of fitted models
allnllk <- unlist(lapply(allm, function(m) m$mod$minimum))
# Index of best fitting model (smallest negative log-likelihood)
whichbest <- which.min(allnllk)
# Best fitting model
k.models[[2]] <- allm[[whichbest]]
allm[[whichbest]]
plot(allm[[whichbest]])
dat<- read.csv("Armadillo Environ Data_NoBurrow.csv", as.is = T)
View(dat)
rm(list=ls(all=TRUE))
setwd("~/Downloads")
dat=read.csv('Armadillo Environ Data_NoBurrow.csv',as.is=T)
hist(dat$dist)
hist(dat$rel.angle)
plot(table(dat$dt),type='h',xlim=c(0,1000))
cond=dat$dt>240 & dat$dt<360 #keep measurements between 4 and 6 minutes apart
dat1=dat[cond,]
cond=dat$dt>=240 & dat$dt=<360 #keep measurements between 4 and 6 minutes apart
dat1=dat[cond,]
dat1$SL=dat1$dist/dat1$dt
cond=dat$dt>=240 & dat$dt<=360 #keep measurements between 4 and 6 minutes apart
dat1=dat[cond,]
cond=dat$dt>240 & dat$dt<360 #keep measurements between 4 and 6 minutes apart
dat1=dat[cond,]
dat1$SL=dat1$dist/dat1$dt
dat1$TA=dat1$rel.angle
hist(dat1$SL)
qtt=quantile(dat1$SL,0.999); qtt
table(dat1$InBurrow)
cond=dat1$SL<qtt & dat1$InBurrow==0 #only animals with sensible SL's and that are not in the burrow
dat1a=dat1[cond,]
plot(y~x,data=dat1a)
dat2=dat1a
#discretize SL
hist(dat2$SL);
quantile(dat2$SL,c(seq(from=0,to=1,length.out=17)))
break.sl=c(seq(from=0,to=0.6,by=0.1),max(dat2$SL))
tmp=cut(dat2$SL,breaks=break.sl); table(tmp)
tmp
table(tmp)
tmp1=as.numeric(tmp); table(tmp1)
dat2$SL1=tmp1
#discretize TA
hist(dat2$TA)
break.ta=seq(from=-pi,to=pi,length.out=11)
tmp=cut(dat2$TA,breaks=break.ta); table(tmp)
tmp1=as.numeric(tmp); table(tmp1)
dat2$TA1=tmp1
#export results
write.csv(dat2,'Armadillo edited.csv',row.names=F)
#export break points
break1=data.frame(breaks=c(break.sl,break.ta),tipo=c(rep('sl',length(break.sl)),
rep('ta',length(break.ta))))
write.csv(break1,'Armadillo edited break points.csv',row.names=F)
break1
rm(list=ls(all=TRUE))
dat=read.csv('Armadillo edited.csv',as.is=T)
#make dummy variables for LULC
table(dat$lulc)
dat$lulc=ifelse(dat$lulc%in%c('Agua','Sede','Cana'),'outros',dat$lulc)
table(dat$lulc)
uni=unique(dat$lulc)
uni=uni[uni!='Campo'] #baseline will be campo
nuni=length(uni)
xmat=matrix(0,nrow(dat),nuni)
for (i in 1:nuni){
cond=uni[i]==dat$lulc
xmat[cond,i]=1
}
unique(xmat)
View(xmat)
colnames(xmat)=uni
#standardize other covariates
nomes=c('slope','ndvi','t.ar','rain')
media1=apply(dat[,nomes],2,mean,na.rm=T)
sd1=apply(dat[,nomes],2,sd,na.rm=T)
for (i in 1:length(nomes)){
dat[,nomes[i]]=(dat[,nomes[i]]-media1[nomes[i]])/sd1[nomes[i]]
}
#get final dataset
dat1=cbind(dat[,c('region','x','y','date','id','SL1','TA1',nomes)],xmat)
write.csv(dat1,'Armadillo edited1.csv',row.names=F)
fim=rbind(media1,sd1)
write.csv(fim,'Armadillo edited1 stats.csv',row.names=F)
rm(list=ls(all=TRUE))
library('MCMCpack')
library('Rcpp')
set.seed(3)
source('mixmov_function.R')
source('mixmov_gibbs.R')
sourceCpp('aux1.cpp')
dat0=read.csv('Armadillo edited1.csv',as.is=T)
dat=data.matrix(dat0[,c('SL1','TA1')])
#prior
alpha=0.1
#initialize parameters
nmaxclust=10
#MCMC stuff
ngibbs=20000
nburn=ngibbs/2
#run gibbs
mod=mixture_movement(dat=dat,alpha=alpha,ngibbs=ngibbs,nmaxclust=nmaxclust,nburn=nburn)
#look at convergence
seq1=1:ngibbs
seq1=nburn:ngibbs
plot(mod$loglikel[seq1], type='l')
mod$theta
# Determine the MAP estimate of the posterior
MAP.iter<- bayesmove::get_MAP_internal(dat = mod$loglikel, nburn = nburn)
theta<- mod$theta[MAP.iter,]
names(theta)<- 1:length(theta)
theta<- sort(theta, decreasing = TRUE)
theta
theta %>% cumsum()  #first 3 states likely (represent 92.1% of all assigned states)
cumsum(theta)
# Store cluster order for plotting and behavioral state extraction
ord<- as.numeric(names(theta))
# Extract bin estimates for each possible state from the `phi` matrix of the model results
behav.res<- get_behav_hist(dat = mod, nburn = nburn, ngibbs = ngibbs, nmaxclust = nmaxclust,
var.names = c("Speed","Turning Angle"),
ord = ord, MAP.iter = MAP.iter)
# Extract bin estimates for each possible state from the `phi` matrix of the model results
behav.res<- bayesmove::get_behav_hist(dat = mod, nburn = nburn, ngibbs = ngibbs, nmaxclust = nmaxclust,
var.names = c("Speed","Turning Angle"),
ord = ord, MAP.iter = MAP.iter)
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = as.factor(behav))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
library(ggplot2)
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = as.factor(behav))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
View(mod)
phi1<- lapply(mod$phi, function(x) x[MAP.iter])
View(phi1)
View(mod)
phi1<- lapply(mod$phi, function(x) x[MAP.iter,])
View(phi1)
apply(dat, 2, max)
apply(dat, 2, max, na.rm=T)
phi1<- lapply(mod$phi, function(x) matrix(x[MAP.iter,], nrow = 10, byrow = T))
View(phi1)
lapply(phi1, function(x) apply(x,1,sum))
lapply(phi1, function(x) apply(x,2,sum))
phi1<- lapply(mod$phi, function(x) matrix(x[MAP.iter,], ncol = 10, byrow = T))
lapply(phi1, function(x) apply(x,2,sum))
plot(phi1[[1]][,1], type = "h")
plot(phi1[[2]][,1], type = "h")
ncol(phi1[[1]])
par(mfrow=c(10,2))
for(i in 1:ncol(phi1[[1]])) {
plot(phi1[[1]][,i], type = "h")
plot(phi1[[2]][,i], type = "h")
}
par(mfrow=c(5,2))
for(i in 1:5) {
plot(phi1[[1]][,i], type = "h")
plot(phi1[[2]][,i], type = "h")
}
par(mfrow=c(3,2))
for(i in 1:3) {
plot(phi1[[1]][,i], type = "h")
plot(phi1[[2]][,i], type = "h")
}
par(mfrow=c(2,2))
for(i in 1:2) {
plot(phi1[[1]][,i], type = "h")
plot(phi1[[2]][,i], type = "h")
}
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = as.factor(behav))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
theta<- mod$theta[MAP.iter,]
theta
names(theta)<- 1:length(theta)
names
theta
theta<- sort(theta, decreasing = TRUE)
theta %>% cumsum()  #first 3 states likely (represent 92.1% of all assigned states)
theta
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = factor(behav, levels = 1:10))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
library('MCMCpack')
library('Rcpp')
set.seed(3)
dat0=read.csv('Armadillo edited1.csv',as.is=T)
dat=data.matrix(dat0[,c('SL1','TA1')])
#prior
alpha=0.1
#initialize parameters
nmaxclust=10
#MCMC stuff
ngibbs=20000
nburn=ngibbs/2
#run gibbs
mod=bayesmove::cluster_obs(dat=dat,alpha=alpha,ngibbs=ngibbs,nmaxclust=nmaxclust,nburn=nburn)
View(dat0)
dat=dat0[,c('id','SL1','TA1')]
set.seed(3)
dat0=read.csv('Armadillo edited1.csv',as.is=T)
dat=dat0[,c('id','SL1','TA1')]
#prior
alpha=0.1
#initialize parameters
nmaxclust=10
#MCMC stuff
ngibbs=20000
nburn=ngibbs/2
#run gibbs
mod=bayesmove::cluster_obs(dat=dat,alpha=alpha,ngibbs=ngibbs,nmaxclust=nmaxclust,nburn=nburn)
View(mod)
seq1=nburn:ngibbs
plot(mod$loglikel[seq1], type='l')
# Determine the MAP estimate of the posterior
MAP.iter<- bayesmove::get_MAP_internal(dat = mod$loglikel, nburn = nburn)
theta<- mod$theta[MAP.iter,]
names(theta)<- 1:length(theta)
theta<- sort(theta, decreasing = TRUE)
theta %>% cumsum()  #first 3 states likely (represent 92.1% of all assigned states)
cumsum(theta)
# Store cluster order for plotting and behavioral state extraction
ord<- as.numeric(names(theta))
# Extract bin estimates for each possible state from the `phi` matrix of the model results
behav.res<- bayesmove::get_behav_hist(dat = mod, nburn = nburn, ngibbs = ngibbs, nmaxclust = nmaxclust,
var.names = c("Speed","Turning Angle"),
ord = ord, MAP.iter = MAP.iter)
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = factor(behav, levels = 1:10))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
library(ggplot2)
# Plot state-dependent distributions
ggplot(behav.res, aes(x = bin, y = prop, fill = factor(behav, levels = 1:10))) +
geom_bar(stat = 'identity') +
labs(x = "\nBin", y = "Proportion\n") +
theme_bw() +
theme(axis.title = element_text(size = 16),
axis.text.y = element_text(size = 14),
axis.text.x.bottom = element_text(size = 12),
strip.text = element_text(size = 14),
strip.text.x = element_text(face = "bold")) +
scale_fill_manual(values = c(viridis::viridis(2), rep("grey35", 8)), guide = FALSE) +
scale_y_continuous(breaks = c(0.00, 0.50, 1.00)) +
# scale_x_continuous(breaks = 1:8) +
facet_grid(behav ~ var, scales = "free_x")
phi1<- lapply(mod$phi, function(x) matrix(x[MAP.iter,], ncol = 10, byrow = T))
lapply(phi1, function(x) apply(x,2,sum))
par(mfrow=c(2,2))
for(i in 1:2) {
plot(phi1[[1]][,i], type = "h")
plot(phi1[[2]][,i], type = "h")
}
